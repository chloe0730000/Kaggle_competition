{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 玉山人工智慧公開挑戰賽2019夏季賽 - 台灣不動產AI神預測 \n\n- House price: Predict sales prices\n- 購買屬於自己的房子，是我們一生中的大事，評估房子的好壞，不僅要考慮房子的大小、屋齡，甚至連附近的生活機能和未來發展性都是一大考量因素。然而在眾多的考量中，什麼才是影響房價高低至關重要的因素呢？"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport scipy.stats as stats\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nprint(\"Train \", train.shape[0], \"sales, and \", train.shape[1], \"features\" )\nprint(\"Test \", test.shape[0], \"sales, and \", test.shape[1], \"features\" )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### EDA\n\nThere are 60,000 instances of training data and 10000 of test data. Total number of attributes equals 235, of which 233 is quantitative, Id (qualitative) and SalePrice. -> all are quantitative except ID\n\n- **Missing Value**: 4 features have missing value in both train and test data: 'village_income_median', 'txn_floor', 'parking_price', 'parking_area'\n\n- **Normal Distributaion on total price**: total_price doesn't follow normal distribution, so before performing regression it has to be transformed -> use standardisation since log total price follow gaussian distribution\n\n- Data is non-linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"quantitative = [f for f in train.columns if train[f].dtypes != 'object']\n#quantitative.remove('total_price')\nqualitative = [f for f in train.columns if train[f].dtypes == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"train data type: \", train.get_dtype_counts())\nprint(\"test data type: \", test.get_dtype_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr_missing = train.isnull().sum()\ntr_missing = tr_missing[tr_missing > 0]\ntr_missing.sort_values(inplace=True)\ntr_missing = pd.DataFrame(tr_missing).reset_index()\ntr_missing = tr_missing.rename(columns = {\"index\": \"features\", 0:\"count\"})\n\nte_missing = test.isnull().sum()\nte_missing = te_missing[te_missing > 0]\nte_missing.sort_values(inplace=True)\nte_missing = pd.DataFrame(te_missing).reset_index()\nte_missing = te_missing.rename(columns = {\"index\": \"features\", 0:\"count\"})\n\n\ndef plot_missing(df_train, df_test):\n    \n    plt.figure(figsize=(15, 5)) \n    \n    plt.subplot(1,2,1)\n    plt.grid(b=None) # no grid line\n    plt.bar(df_train[\"features\"], df_train[\"count\"])\n    plt.title(\"missing value in train data\", loc='center')\n    plt.xticks(rotation=45)\n    \n    plt.subplot(1,2,2)\n    plt.grid(b=None) # no grid line\n    plt.bar(df_test[\"features\"], df_test[\"count\"], color=(0.2, 0.4, 0.6, 0.6))\n    plt.title(\"missing value in test data\", loc='center')\n    plt.xticks(rotation=45)\n    \n    plt.show()\n    \nplot_missing(tr_missing, te_missing)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['total_price']\nlog_price = train[\"total_price\"].apply(lambda row: math.log(row))\n\nf, axes = plt.subplots(1, 3, figsize=(15,5))\n\nsns.distplot(y, kde=False, fit=stats.johnsonsu, ax = axes[0])\nplt.title('Johnson SU')\n\nsns.distplot(y, kde=False, fit=stats.norm, ax = axes[1])\nplt.title('Normal Distribution')\n\nsns.distplot(log_price, kde=False, fit=stats.norm, ax = axes[2])\nplt.title('Normal Distribution with log transform for total price (train)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Shapiro-Wilk test evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution\nwarnings.filterwarnings(\"ignore\")\n\ntest_normality = lambda x: stats.shapiro(x.fillna(0))[1] < 0.01\nnormal = pd.DataFrame(train[quantitative])\nnormal = normal.apply(test_normality)\n\n# not reject null hypothesis -> features with normal distribution\nprint(\"features with normal distribution: \\n\", list(pd.DataFrame(normal[normal==False]).reset_index()[\"index\"])) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data is non-linear (skip first -> take too long)\n\n# features = [col for col in train.columns if col not in [\"building_id\", \"total_price\"]]\n# model = TSNE(n_components=2, random_state=0, perplexity=50)\n\n# X = train[features].fillna(0).values\n# tsne = model.fit_transform(X)\n\n# std = StandardScaler()\n# s = std.fit_transform(X)\n\n# pca = PCA(n_components=30)\n# pca.fit(s)\n# pc = pca.transform(s)\n# kmeans = KMeans(n_clusters=5)\n# kmeans.fit(pc)\n\n# fr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})\n# sns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)\n# print(np.sum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle Missing Value\n- village_income_median\n- txn_floor\n- parking_price\n- parking_area\n\nNote: \n1. whether should comebine train and test to replace the missing value \n2. strong assumption on if park_way==2 -> parking_area & parking_price = 0 <br/>\n    (found: parking_way==2 -> parking_price=na and parking_area=na)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"total_price\"] = -1\nall_df = pd.concat([train, test])\n\nprint(\"# of missing value in village income median (all): \", all_df[\"village_income_median\"].isnull().sum())\nprint(\"# of missing value in village income median (train): \", train[\"village_income_median\"].isnull().sum())\nprint(\"# of missing value in village income median (test): \", test[\"village_income_median\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------------------\n# remove column with no variation\n#----------------------------------------------\n\nprint(\"data shape: \", all_df.shape)\nall_df = all_df.loc[:,all_df.apply(pd.Series.nunique) != 1]\nprint(\"data shape after removing columns with no variation: \", all_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get possiblely categorical variable\ncategorical_var=[]\nfor col in all_df:\n    if len(all_df[col].unique())>1 and all_df[col].dtypes==\"int64\":\n        #print (col, \"-> unique value: \", train[col].unique())\n        categorical_var.append(col)\nprint(categorical_var)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------------------\n# missing value in village_income_median\n#----------------------------------------------\n\nmissing_income_village = np.unique(all_df[pd.isnull(all_df[\"village_income_median\"])][[\"village\"]])\nprint(\"# of village missing median income: \", len(missing_income_village))\n\nmissing_income_village_tr = np.unique(train[pd.isnull(train[\"village_income_median\"])][[\"village\"]])\nprint(\"# of village missing median income (train): \", len(missing_income_village_tr))\n\na = all_df[all_df[\"village\"].isin(missing_income_village)][[\"village\", \"village_income_median\"]]\nvillage_have_income_replace = np.unique(a[pd.notnull(a[\"village_income_median\"])][[\"village\"]])\nprint(\"# of village can be replace by mean village income: \", len(village_have_income_replace))\nprint(\"# of village no replacement: \", len(set(missing_income_village)-set(village_have_income_replace)))\n\n\n# replace village_have_income_replace with median\nfor item in village_have_income_replace:\n    all_village_median_income = all_df.groupby(by=\"village\")[\"village_income_median\"].agg(\"median\")\n    all_df[\"village_income_median\"] = all_df.apply(lambda row:  all_village_median_income[item]\n                                   if (row[\"village\"]==item and pd.isnull(row[\"village_income_median\"])) \n                                   else row[\"village_income_median\"], axis=1)\n\n# replace those cannot find with median of town\nfor item in list(set(all_df[\"town\"])):\n    all_town_median_income = all_df.groupby(by=\"town\")[\"village_income_median\"].agg(\"median\")\n    all_df[\"village_income_median\"] = all_df.apply(lambda row:  all_town_median_income[item]\n                                   if (row[\"town\"]==item and pd.isnull(row[\"village_income_median\"])) \n                                   else row[\"village_income_median\"], axis=1)\n\n# check no na\nprint(\"# of missing value in village_income_median \", all_df[\"village_income_median\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------------------\n# missing value in txn_floor\n#----------------------------------------------\n\n# city with na txn_floor\ncity_na_floor = np.unique(all_df[pd.isnull(all_df[\"txn_floor\"])][[\"city\"]])\n\nb = all_df[all_df[\"city\"].isin(city_na_floor)]\ncity_with_floor = np.unique(b[pd.notnull(b[\"txn_floor\"])][[\"city\"]])\n\n# check whether all na floor can be replace with floor data by existing data\nprint(\"# of city with no txn_floor data: \", len(set(city_na_floor) - set(city_with_floor)))\n\n# replace those na in txn_floor with the existing data\n\ncity_txn_floor_data = all_df.groupby(by=[\"city\", \"txn_floor\"]).size().reset_index().rename(columns={0: \"count\"})\ncity_txn_floor_data[\"total_count\"] = city_txn_floor_data[\"city\"].map(city_txn_floor_data.groupby(\"city\")[\"count\"].agg(\"sum\"))\n\ncity_txn_floor_data[\"weight\"] = city_txn_floor_data[\"count\"]/city_txn_floor_data[\"total_count\"]\ncity_txn_floor_data[\"max_count\"] = city_txn_floor_data[\"city\"].map(city_txn_floor_data.groupby(\"city\")[\"count\"].agg(\"max\"))\n\ncity_txn_floor_data = city_txn_floor_data[city_txn_floor_data[\"count\"]==city_txn_floor_data[\"max_count\"]][[\"city\", \"txn_floor\"]]\ncity_txn_floor_data = city_txn_floor_data.rename(columns={\"txn_floor\":\"txn_floor_n\"})\n\nall_df = pd.merge(all_df, city_txn_floor_data, how=\"left\", on = \"city\")\n\ndef replace_txn_floor(row):\n    if pd.isnull(row[\"txn_floor\"]):\n        row[\"txn_floor\"] = row[\"txn_floor_n\"]\n    else:\n        row[\"txn_floor\"]\n    return row[\"txn_floor\"]\nall_df[\"txn_floor\"] = all_df.apply(replace_txn_floor, axis=1)\ndel all_df[\"txn_floor_n\"]\n\n# check no na\nprint(\"# of missing value in txn_floor \", all_df[\"txn_floor\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------------------\n# missing value in parking_price\n#----------------------------------------------\n\n# parking_way==2 -> parking_price=na and parking_area=na\nprint(all_df.groupby(by=[\"parking_way\"])[\"parking_price\"].agg(\"mean\"))\nprint(all_df[(all_df[\"parking_way\"]==2)&(pd.notnull(all_df[\"parking_area\"]))&(pd.notnull(all_df[\"parking_price\"]))].shape)\n\ndef replace_parking_price(row):\n    if pd.isnull(row[\"parking_price\"]) and row[\"parking_way\"]==2:\n        row[\"parking_price\"] = 0\n    else:\n        row[\"parking_price\"]\n    return row[\"parking_price\"]\n\nall_df[\"parking_price\"] = all_df.apply(replace_parking_price, axis=1)\nprint(\"# of missing value in parking price: \", all_df[\"parking_price\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------------------\n# missing value in parking_area\n#----------------------------------------------\n\n# parking_way==2 -> parking_price=na and parking_area=na\n\ndef replace_parking_area(row):\n    if pd.isnull(row[\"parking_area\"]) and row[\"parking_way\"]==2:\n        row[\"parking_area\"] = 0       \n    else:\n        row[\"parking_area\"]\n    return row[\"parking_area\"]\nall_df['parking_area'] = all_df.apply(replace_parking_area, axis=1)\n\nall_df['parking_area'] = all_df.groupby(by=[\"village\", \"parking_way\"])['parking_area'].transform(lambda x: x.fillna(x.median()))\nall_df['parking_area'] = all_df.groupby(by=[\"town\", \"parking_way\"])['parking_area'].transform(lambda x: x.fillna(x.median()))                                                                                                  \nall_df['parking_area'] = all_df.groupby(by=[\"city\", \"parking_way\"])['parking_area'].transform(lambda x: x.fillna(x.median()))                                           \nall_df[\"parking_area\"] = all_df[\"parking_area\"].fillna(0)\n\nprint(\"# of missing value in parking area: \", all_df[\"parking_area\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#----------------------------------------------\n# Standardisation continuous data (note: should separate train and test?)\n#----------------------------------------------\n\nall_df.drop(\"building_id\", axis=1, inplace=True)\ntrain1 = all_df[:len(train)]\ntest1 = all_df[len(train):]\n\n# exclude target variable\nstandardise_list = list(set(all_df.columns)-set(categorical_var))\nstandardise_list.remove(\"total_price\")\nstandarize = lambda x: (x-x.mean()) / x.std()\n\nfor col in standardise_list:\n    train1[col] = standarize(train1[col])\n    test1[col] = standarize(test1[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make categorical variable one hot encode\n#all_df2 = pd.get_dummies(all_df, columns = categorical_var)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check features correlation with target (skip first)\n\n# corr_sale=train1.corr()[\"total_price\"]\n\n# fig, ax = plt.subplots(figsize = (50, 50))\n# #corr_sale[np.argsort(corr_sale, axis=0)[::-1]].plot(kind='barh')\n# corr_sale[(corr_sale>0.05)].sort_values(ascending= False).plot(kind='barh')\n# plt.tick_params(labelsize=12)\n# plt.ylabel(\"Pearson correlation\",size=12)\n# plt.title('Correlated features with Sale Price', size=13)\n# plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make categorical variable as string to avoid being treated as numeric variable\nfor col in categorical_var:\n    train1[col] = train1[col].astype(str)\n    test1[col] = test1[col].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2 = train1.drop('total_price', axis=1)\ntest2 = test1.drop(\"total_price\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train2, train1[\"total_price\"], test_size=0.2, random_state=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)\nprint(lm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lm.intercept_)\n#print(lm.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = lm.predict(X_test)\npredictions= predictions.reshape(-1,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, predictions))\nprint('MSE:', metrics.mean_squared_error(y_test, predictions))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = test[[\"building_id\"]]\noutput[\"total_price\"] = lm.predict(test2).reshape(-1,1)\noutput.to_csv(\"submission_linear_reg.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#--------- till here"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.boxplot(x=\"city\", y='total_price', data=train1, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr_sale=train_1.corr()[\"total_price\"]\n\n# fig, ax = plt.subplots(figsize = (50, 50))\n# corr_sale[np.argsort(corr_sale, axis=0)[::-1]].plot(kind='barh')\n# plt.tick_params(labelsize=12)\n# plt.ylabel(\"Pearson correlation\",size=12)\n# plt.title('Correlated features with Sale Price', size=13)\n# plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# corr = train_1.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n# f, ax = plt.subplots(figsize=(11, 11))\n# sns.heatmap(corr, vmin=-0.8, vmax=0.8, square=True)\n# f.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check fake data in test\n\n# test1 = test.drop(['building_id'], axis=1)\n# test1.shape\n\n# unique_samples = []\n# unique_count = np.zeros_like(test1)\n\n# feature = [col for col in train.columns if col not in [\"building_id\", \"total_price\"]]\n\n# for i in range(len(feature)):\n#     _, index_, count_ = np.unique(test1.loc[:, feature[i]], return_counts=True, return_index=True)\n#     unique_count[index_[count_ == 1], i] += 1\n\n# # Samples which have unique values are real the others are fake\n# real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n# synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n\n# print(\"# of real data: \", len(real_samples_indexes))\n# print(\"# of fake data: \", len(synthetic_samples_indexes))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}