{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# import package\nimport numpy as np \nimport pandas as pd \nimport gc\nimport datetime\nimport warnings\nimport seaborn as sns\nimport math\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.stats import norm, rankdata\n\n# scaling\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Dimension Reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import MDS\nfrom sklearn.cluster import KMeans\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Evaluation\nfrom tqdm import tqdm_notebook\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, ExtraTreesRegressor                   \nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom xgboost import XGBClassifier\n#import xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom vecstack import stacking\n\n\n# bayesian optimisation\nimport hyperopt\nfrom hyperopt import hp\nfrom hyperopt import tpe\nfrom hyperopt import Trials\nfrom hyperopt import fmin\nimport csv\nfrom timeit import default_timer as timer\nfrom hyperopt import STATUS_OK, hp, tpe, Trials, fmin\nimport ast\n\nfrom numpy.random import RandomState\n\nwarnings.filterwarnings('ignore')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# read data\ndata_train = pd.read_csv(\"../input/train.csv\", encoding = 'utf-8-sig')\ndata_test = pd.read_csv(\"../input/test.csv\", encoding = 'utf-8-sig')\ndata_sample = pd.read_csv(\"../input/sample_submission.csv\", encoding = 'utf-8-sig')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b4388cd2f7a10c7d4b861e06ff92439bc71e38a3"
      },
      "cell_type": "markdown",
      "source": "## Check Data\n- 1. data shape\n- 2. columns\n- 3. missing value\n- 4. check data type -> model cannot deal with type except float"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5849eb9af6224c1527b1c5df61b9ffadc698eda5"
      },
      "cell_type": "code",
      "source": "# [1] data shape\n\nprint(\"Train data shape:\", data_train.shape)\nprint(\"Test data shape:\", data_test.shape)\n\n# [2] check columns: train data get 1 column more than the test data\nprint(\"check columns in train data but not in test data: \\n\",\n      data_train.columns[~data_train.columns.isin(data_test.columns)])\nprint(\"check columns in test data but not in train data: \\n\",\n      data_test.columns[~data_test.columns.isin(data_train.columns)])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cb65e888476ced353402d3b659a20333b675e237"
      },
      "cell_type": "code",
      "source": "# [3] check missing value\ndata_check_missing_val = pd.DataFrame(data_train.isnull().sum()/data_train.isnull().count()).rename(columns={0:\"percentage of missing value\"}).reset_index().rename(columns={\"index\":\"feature_name\"})\nprint(\"feature with missing value: \", data_check_missing_val[data_check_missing_val[\"percentage of missing value\"]!=0].feature_name)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b3e2395917a9872f1aaab7512a635e3c161f03df"
      },
      "cell_type": "code",
      "source": "# [4] check column types -> check data type before scaling\nprint(\"train data columns that are not float type: \",\n      [col for col in data_train.columns if data_train[col].dtype!=float])\n\nprint(\"test data columns that are not float type: \",\n      [col for col in data_test.columns if data_test[col].dtype!=float])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9735f39b456dd0f397ef5cf4813a53bae5dae0c"
      },
      "cell_type": "code",
      "source": "print(\"---> train data information <--- \\n\")\nprint(data_train.info())\nprint(\"---> test data information <--- \\n\")\nprint(data_test.info())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a18cd50c3de9a0d061cd16c317381c78c682a2f1"
      },
      "cell_type": "markdown",
      "source": "## Explore Data\n\n- 1. summary statistics -> standard deviation & mean too high\n- 2. check imbalance data\n- 3. check feature correlation\n- 4. check duplicate within a column"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "4736eeb6650f89f9609abf7b35ea2d959b9a40d3"
      },
      "cell_type": "code",
      "source": "# [1]\nplt.title(\"train data: distribution of standard deviation\")\nsns.distplot(data_train.describe().loc['std'])\nplt.show()\n\nplt.title(\"train data: distribution of mean\")\nsns.distplot(data_train.describe().loc['mean'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "213697c22f025481a56131c15d08d41af9501b11"
      },
      "cell_type": "code",
      "source": "plt.title(\"test data: distribution of standard deviation\")\nsns.distplot(data_test.describe().loc['std'])\nplt.show()\n\nplt.title(\"test data: distribution of mean\")\nsns.distplot(data_test.describe().loc['mean'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec5c46be0c2c3102b978def8eee678f710998751",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# [2] check imbalance data\nprint(\"--> % of target data in the data <-- \\n\", \n      data_train[\"target\"].sum()/len(data_train))\n\nsns.countplot(data_train['target'])\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5ce4c87b3bba3bb415bd3d787bc165075486612d"
      },
      "cell_type": "code",
      "source": "# [3] check feature correlation\nfeatures = [col for col in data_train.columns if col not in [\"ID_code\", \"target\"]]\ncorrelations = data_train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f2c3a3dce7958e711fa44366f70528f737f9ec24"
      },
      "cell_type": "code",
      "source": "correlations[:10]\ncorrelations[:-11:-1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba8a5a25679a929853211f9aaaef63ed562f05d6"
      },
      "cell_type": "code",
      "source": "# [4] check duplicate\nval_of_col_train = []\nnum_of_dup_train = []\nval_of_col_test = []\nnum_of_dup_test = []\n\nfor col in features:\n    val_of_col_train.append(data_train[col].value_counts().nlargest(1).index.values.tolist())\n    num_of_dup_train.append(data_train[col].value_counts().nlargest(1).values.tolist())\n    val_of_col_test.append(data_test[col].value_counts().nlargest(1).index.values.tolist())\n    num_of_dup_test.append(data_test[col].value_counts().nlargest(1).values.tolist())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcb32bdd50fcaa5ab21556a03b2663a8efb80cf3"
      },
      "cell_type": "code",
      "source": "data_check_duplicates_train = pd.concat([pd.DataFrame(features),\n                                   pd.DataFrame(val_of_col_train),\n                                   pd.DataFrame(num_of_dup_train)], axis=1)\ncol_list = [\"col_name\", \"most_freq_appearance_value\", \"# of duplicates\"]\ndata_check_duplicates_train.columns = col_list\n\ndata_check_duplicates_test = pd.concat([pd.DataFrame(features),\n                                   pd.DataFrame(val_of_col_test),\n                                   pd.DataFrame(num_of_dup_test)], axis=1)\ndata_check_duplicates_test.columns = col_list",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d1cc194e01110b4bfe1bfe686a8e5b4088ec1178"
      },
      "cell_type": "code",
      "source": "print(\"train data: top 10 col with most duplicate values\")\ndata_check_duplicates_train.sort_values(by=\"# of duplicates\", ascending = False)[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70584e14b4cc1aba7dfd807cf6e3a4eb6c06b1d9"
      },
      "cell_type": "code",
      "source": "print(\"test data: top 10 col with most duplicate values\")\ndata_check_duplicates_test.sort_values(by=\"# of duplicates\", ascending = False)[:10]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "49e2f4e8754cd282d5d3d1ca6a0a76a7318b8b09"
      },
      "cell_type": "markdown",
      "source": "## adding features\n- get sum, min, max, ... of each row"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "758aaee6d614d5eb1609d85dcc0c6e434d68bfff"
      },
      "cell_type": "code",
      "source": "for df in [data_train, data_test]:\n    df['sum'] = df[features].sum(axis=1)  \n    df['min'] = df[features].min(axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['mean'] = df[features].mean(axis=1)\n    df['std'] = df[features].std(axis=1)\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurtosis(axis=1)\n    df['med'] = df[features].median(axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0cc60b185dd994a5e6bf51c75dc406aa818f9a48"
      },
      "cell_type": "code",
      "source": "# for df in [data_train, data_test]:\n#     for col in features:\n#         df[col+'_s'] = df[col]**2\n#         df[col+'_c'] = df[col]**3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "57dc900d0e85ab7a88a56df0b4d877483bef0d2b"
      },
      "cell_type": "markdown",
      "source": "## Model - LGBM\n- tree based model no need scaling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9d37fd8e4a2dc70ecf89ba4f252ce70e35c1c95"
      },
      "cell_type": "code",
      "source": "target = data_train['target']\nID = data_test[\"ID_code\"]\nfeatures = new_features = [col for col in data_train.columns if col not in [\"ID_code\", \"target\"]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82ff416b314a3f6849a045ee241945b1e5b54a2a",
        "_kg_hide-input": false,
        "_kg_hide-output": false
      },
      "cell_type": "code",
      "source": "# print(---> Grid Search <---)\n# param = {\n#     'bagging_freq': 5,\n#     'bagging_fraction': 0.4,\n#     'boost_from_average':'false',\n#     'boost': 'gbdt',\n#     'feature_fraction': 0.05,\n#     'learning_rate': 0.01,\n#     'max_depth': -1,  \n#     'metric':'auc',\n#     'min_data_in_leaf': 80,\n#     'min_sum_hessian_in_leaf': 10.0,\n#     'num_leaves': 13,\n#     'num_threads': 8,\n#     'tree_learner': 'serial',\n#     'objective': 'binary', \n#     'verbosity': 1\n# }\n\n# %%time\n# # gridsearch -> use LGBMClassifier to get grid search result\n# train_x, valid_x, train_y, valid_y = train_test_split(data_train[features], target, test_size=0.2, random_state=0)   # 分训练集和验证集\n# train = lgb.Dataset(train_x, train_y)\n# valid = lgb.Dataset(valid_x, valid_y, reference=train)\n# parameters = {'min_data_in_leaf': [60,70,80,90]}\n              \n\n# gbm = lgb.LGBMClassifier(\n#     boosting_type='gbdt',\n#     objective = 'binary',\n#     metric = 'auc',\n#     verbose = 0,\n#     learning_rate = 0.05,\n#     num_leaves = 35,\n#     feature_fraction=0.05,\n#     bagging_fraction= 0.8,\n#     bagging_freq= 5,\n#     min_data_in_leaf = 80)\n\n# # 有了gridsearch我们便不需要fit函数\n# gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='roc_auc', cv=5)\n# gsearch.fit(train_x, train_y)\n\n# print(\"Best score: %0.3f\" % gsearch.best_score_)\n# print(\"Best parameters set:\", gsearch.best_estimator_.get_params())\n# #best_parameters = gsearch.best_estimator_.get_params()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e417e07923ec07dd177d2e44d2d1aac41856e2a"
      },
      "cell_type": "code",
      "source": "# print(\"---> Bayesian Optimisation step 1 find object function <---\")\n\n# train_all_data = lgb.Dataset(data_train[features],target)\n\n# def hyperopt_objective(params):\n\n#     # [optional]: need to use for features that only takes integer\n#     for parameter_name in ['min_data_in_leaf', 'num_leaves']:\n#         params[parameter_name] = int(params[parameter_name])    \n    \n#     model = lgb.LGBMClassifier(class_weight = 'balanced', boosting_type = 'gbdt',\n#                            objective = 'binary', metric = 'auc',\n#                            verbosity = 1, tree_learner = 'serial', \n#                            bagging_freq = 5,\n#                            learning_rate = params['learning_rate'],\n#                            num_leaves = params['num_leaves'],\n#                            min_data_in_leaf = params['min_data_in_leaf'],                           \n#                            bagging_fraction = params['bagging_fraction'],\n#                            feature_fraction = params['feature_fraction'],\n#                            min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf']\n#                           )\n    \n#     res = lgb.cv(model.get_params(), train_all_data, num_boost_round = 1000, \n#                  nfold = 10, metrics='auc',early_stopping_rounds=100, seed = 0)\n\n#     return np.max(res['auc-mean'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6d673e72b2965d2f2303b6822576a161c701d34"
      },
      "cell_type": "code",
      "source": "# print(\"---> Bayesian Optimisation step 2 define space <---\")\n\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n\n# # list all potential parameters for lightgbm\n# params_space = {\n#     'num_leaves': hp.quniform('num_leaves', 10, 40, 5),\n#     'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n#     'min_data_in_leaf': hp.quniform('min_data_in_leaf', 10, 100, 5),\n#     'bagging_fraction': hp.uniform('bagging_fraction', 0.01, 1.0),\n#     'feature_fraction': hp.uniform('feature_fraction', 0.01, 1.0),\n#     'min_sum_hessian_in_leaf': hp.uniform('min_sum_hessian_in_leaf', 0, 10)\n#     #'bagging_freq': hp.uniform('bagging_freq', 0.0, 1.0),\n#     #'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n#     #'min_child_samples': hp.quniform('min_child_samples', 20, 25, 5),\n#     #'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n#     #'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n#     #'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n#     #'max_depth': hyperopt.hp.randint('max_depth', 6),\n#     }\n\n\n\n# print(\"---> Bayesian Optimisation record results <---\")\n# trials = hyperopt.Trials()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96a5ff2450f5bb1b79c67056d7d0cbccd3fb5ee7"
      },
      "cell_type": "code",
      "source": "# %%time\n\n# print(\"---> Bayesian Optimisation get the results <---\")\n\n# best = hyperopt.fmin(\n#     hyperopt_objective,\n#     space=params_space,\n#     algo=hyperopt.tpe.suggest,\n#     max_evals=50,\n#     trials=trials,\n#     rstate=RandomState(123)\n#     )\n\n# print(best)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a448ecf8aeddf241b173a5e32cd868469ae6f3a"
      },
      "cell_type": "code",
      "source": "# print(\"---> using bayesian optimisation results automatically <---\")\n\n# param = {\n#     'tree_learner': 'serial',\n#     'objective': 'binary', \n#     'verbosity': 1,\n#     'num_threads': 8,\n#     'boost_from_average':'false',\n#     'boost': 'gbdt',    \n#     'metric':'auc',\n#     'max_depth': -1,  \n#     'bagging_freq': 5,  \n#     'bagging_fraction': best['bagging_fraction'],\n#     'feature_fraction': best['feature_fraction'],\n#     'learning_rate': best['learning_rate'],\n#     'min_data_in_leaf': int(best['min_data_in_leaf']),\n#     'num_leaves': int(best['num_leaves']),\n#     'min_sum_hessian_in_leaf': best['min_sum_hessian_in_leaf']\n#     }",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ba1d52dbb4dca2a0c1f8d54141d82211f38390c"
      },
      "cell_type": "code",
      "source": "# # original (for cases that don't add extra features -> from kaggle kernel)\n# # if use this for new model with extra features -> cv score: 0.89992\n# param = {\n#     'bagging_freq': 5,\n#     'bagging_fraction': 0.4,\n#     'boost_from_average':'false',\n#     'boost': 'gbdt',\n#     'feature_fraction': 0.05,\n#     'learning_rate': 0.01,\n#     'max_depth': -1,  \n#     'metric':'auc',\n#     'min_data_in_leaf': 80,\n#     'min_sum_hessian_in_leaf': 10.0,\n#     'num_leaves': 13,\n#     'num_threads': 8,\n#     'tree_learner': 'serial',\n#     'objective': 'binary', \n#     'verbosity': 1\n# }",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f3a447f2b9acd945eab31ec9be97cf987295b27"
      },
      "cell_type": "code",
      "source": "print(\"---> skip the optimisation process directly use the result <---\")\n\nparam = {\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1,\n    'num_threads': 8,\n    'boost_from_average':'false',\n    'boost': 'gbdt',    \n    'metric':'auc',\n    'max_depth': -1,  \n    'bagging_freq': 5,  \n    'bagging_fraction': 0.9987097091611788,\n    'feature_fraction': 0.7849096588464135,\n    'learning_rate': 0.0100061504279982,\n    'min_data_in_leaf': 40,\n    'num_leaves': 30,\n    'min_sum_hessian_in_leaf': 3.8084319606422166\n    }",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9c0f36ed8d3ef177e4363d3f7795be0eec66066c",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "%%time\n\n# use 5 fold to check since run so slow\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=0)\noof = np.zeros(len(data_train))\npredictions = np.zeros(len(data_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(data_train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    \n    trn_data = lgb.Dataset(data_train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(data_train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], \n                    verbose_eval=1000, early_stopping_rounds = 3000)\n    \n    # get 10 oof data -> cuz cv=10\n    oof[val_idx] = clf.predict(data_train.iloc[val_idx][features], \n                               num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(data_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22dfc0f9dfa3e78669c12a3b9c02ffbb83f6d76d"
      },
      "cell_type": "code",
      "source": "cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9e6cf476c101e09bea78d9914bb7b6733837787"
      },
      "cell_type": "code",
      "source": "submit = pd.DataFrame({\"ID_code\":data_test[\"ID_code\"].values})\nsubmit[\"target\"] = predictions\nsubmit.to_csv(\"bayesian_optimisation_lgbm.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}